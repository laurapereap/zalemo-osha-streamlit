# -*- coding: utf-8 -*-
"""train_model.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1MFuG7Of7_sGtibvGZ_oVnJDS89oTFuOO
"""

# CONFIGURACI√ìN GLOBAL
import sys, os, re, json, math, textwrap
import numpy as np
import pandas as pd
from pathlib import Path
from datetime import datetime

RANDOM_STATE = 42

DATA_PATH = Path("/content/January2015toOctober2024.csv")

# Carpeta de salidas (modelos, reportes r√°pidos, etc.)
OUT_DIR = Path("/content/salidas_osha")
OUT_DIR.mkdir(parents=True, exist_ok=True)

def sep(title=""):
    print("\n" + "="*80)
    if title:
        print(title)
        print("="*80)

# CARGA ROBUSTA CON ESTRATEGIAS

import csv
from pathlib import Path

sep("Carga robusta: m√∫ltiples estrategias")

def try_read_csv(path):
    attempts = []

    # A1) dtype=str + engine c (r√°pido). Suelen fallar los CSV "rotos".
    attempts.append(dict(
        desc="A1: engine='c', dtype=str",
        kwargs=dict(dtype=str, low_memory=False)
    ))

    # A2) engine='python' + dtype=str (m√°s tolerante con comillas y saltos)
    attempts.append(dict(
        desc="A2: engine='python', dtype=str",
        kwargs=dict(dtype=str, engine="python")
    ))

    # A3) engine='python' + sep auto + on_bad_lines='warn' (acepta l√≠neas raras, no las salta)
    #  Nota: on_bad_lines='warn' intentar√° parsear; si no puede, avisa y omite esas l√≠neas.
    attempts.append(dict(
        desc="A3: engine='python', sep autodetect, on_bad_lines='warn'",
        kwargs=dict(dtype=str, engine="python", sep=None, on_bad_lines="warn")
    ))

    # A4) engine='python' + escapechar='\\' + doublequote=True (t√≠pico cuando hay comillas internas)
    attempts.append(dict(
        desc="A4: engine='python' + escapechar, doublequote",
        kwargs=dict(dtype=str, engine="python", escapechar="\\", doublequote=True, on_bad_lines="warn")
    ))

    last_err = None
    for i, att in enumerate(attempts, 1):
        try:
            print(f"\n-> Intento {i}: {att['desc']}")
            df_tmp = pd.read_csv(path, **att["kwargs"])
            print("OK:", df_tmp.shape)
            return df_tmp, att['desc']
        except Exception as e:
            print("Fall√≥:", type(e).__name__, "-", e)
            last_err = e

    raise last_err

assert DATA_PATH.exists(), f"No encuentro el archivo en {DATA_PATH}. Corrige DATA_PATH."

df_raw, load_strategy = try_read_csv(DATA_PATH)

sep("Resumen de carga")
print("Estrategia usada:", load_strategy)
print("Filas x Columnas:", df_raw.shape)

display(df_raw.head(3))

# Guardamos una copia "tal cual" le√≠da para trazabilidad
RAW_COPY = OUT_DIR / "raw_loaded_copy.csv"
df_raw.to_csv(RAW_COPY, index=False)
print(f"Copia de lo cargado guardada en: {RAW_COPY}")

# NORMALIZAR NOMBRES Y DETECTAR COLUMNAS

sep("Normalizaci√≥n de nombres de columnas")

def normalize_cols(cols):
    norm = []
    for c in cols:
        c2 = re.sub(r'[^a-zA-Z0-9]+', '_', c.strip()).strip('_').lower()
        norm.append(c2)
    return norm

df = df_raw.copy()
df.columns = normalize_cols(df.columns)

display(pd.DataFrame({"original": df_raw.columns, "normalizado": df.columns}))

# Funci√≥n auxiliar para encontrar columnas por posibles alias
def find_col(candidates, required=False):
    for cand in candidates:
        for col in df.columns:
            if col == cand or col.endswith("_" + cand) or cand in col:
                return col
    if required:
        raise ValueError(f"No se encontr√≥ ninguna de las columnas {candidates}")
    return None

# Intentamos localizar columnas t√≠picas
date_col   = find_col(["eventdate", "date", "fecha", "event_date"], required=True)
title_col  = find_col(["eventtitle", "title", "titulo"], required=False)  # puede no existir
desc_col   = find_col(["eventdescription", "description", "descripcion", "narrative", "desc"], required=False)
state_col  = find_col(["state", "province", "estado"], required=False)
industry_col = find_col(["industry", "naics_title", "naicsdescription", "sector"], required=False)
naics_col  = find_col(["naics", "naics_code"], required=False)
hospitalized_col = find_col(["hospitalized", "hospitalizacion"], required=False)
amputation_col   = find_col(["amputation", "amputacion"], required=False)

print("Columna fecha:", date_col)
print("Columna t√≠tulo (EventTitle):", title_col)
print("Columna descripci√≥n:", desc_col)
print("Columna estado:", state_col)
print("Columna industria:", industry_col)
print("Columna NAICS:", naics_col)
print("Columna Hospitalized:", hospitalized_col)
print("Columna Amputation:", amputation_col)

# FECHAS, NULOS, DUPLICADOS

sep("Parsing de fechas")

df[date_col] = pd.to_datetime(df[date_col], errors="coerce")
n_before = df.shape[0]
df = df[~df[date_col].isna()].copy()
print(f"Filtrado filas sin fecha v√°lida: {n_before} -> {df.shape[0]}")

sep("Duplicados")
# Definimos una llave razonable para duplicados (aj√∫stala si tienes un ID)
dup_key = [date_col] + [c for c in [title_col, desc_col, state_col] if c]
before = df.shape[0]
df = df.drop_duplicates(subset=dup_key)
print(f"Eliminados duplicados: {before - df.shape[0]} (llave: {dup_key})")

sep("Resumen de nulos por columna (top 20)")
nulls = df.isna().mean().sort_values(ascending=False).head(20)
display(nulls.to_frame("pct_nulls"))

sep("Rango temporal")
print(df[date_col].min(), "‚Üí", df[date_col].max())

# Guardamos una versi√≥n "limpia m√≠nima"
df_min = df.copy()
df_min_path = OUT_DIR / "df_min.csv"
df_min.to_csv(df_min_path, index=False)
print(f"Guardado {df_min_path}")

# LIMPIEZA DE TEXTO

sep("Limpieza de texto")

# Elegimos columna de texto principal (prefiere descripci√≥n; sino t√≠tulo)
text_col = desc_col or title_col
assert text_col is not None, "No encuentro columna de texto (descripci√≥n o t√≠tulo). Revisa alias."

def clean_text(s):
    if pd.isna(s): return ""
    s = str(s).lower()
    s = re.sub(r"\s+", " ", s)
    s = re.sub(r"[^a-z0-9 \-_/]", " ", s)  # mantenemos algunos caracteres √∫tiles
    return s.strip()

df_min[text_col] = df_min[text_col].map(clean_text)

# Quitamos filas sin texto significativo
before = df_min.shape[0]
df_min = df_min[df_min[text_col].str.len() > 10].copy()
print(f"Filtrado texto muy corto: {before} -> {df_min.shape[0]}")
display(df_min[[date_col, text_col]].head(3))

# EDA BREVE

import matplotlib.pyplot as plt

sep("Eventos por a√±o")
df_min["year"] = df_min[date_col].dt.year
year_counts = df_min["year"].value_counts().sort_index()
display(year_counts)

plt.figure(figsize=(10,4))
year_counts.plot(kind="bar")
plt.title("Eventos por a√±o")
plt.xlabel("A√±o"); plt.ylabel("N√∫mero de eventos")
plt.tight_layout(); plt.show()

if state_col in df_min.columns:
    sep("Top 15 estados/provincias")
    display(df_min[state_col].value_counts().head(15))

if industry_col in df_min.columns:
    sep("Top 15 industrias")
    display(df_min[industry_col].value_counts().head(15))

if (hospitalized_col in df_min.columns) or (amputation_col in df_min.columns):
    sep("Severidades (si disponibles)")
    for c in [hospitalized_col, amputation_col]:
        if c and c in df_min.columns:
            print(f"\n{c}")
            display(df_min[c].value_counts(dropna=False))

# DEFINIR FAMILIAS SIMPLIFICADAS

sep("Definir familias del evento")

def default_family_rules(s):
    """
    Reglas simples por keywords. Ajusta libremente para tu dataset.
    Devuelve una 'familia' m√°s general a partir del t√≠tulo del evento.
    """
    s = str(s).lower()
    rules = [
        ("fall",           ["fall", "slip", "trip", "ladder", "scaffold", "elevation"]),
        ("struck_by",      ["struck", "hit by", "impact", "collision", "run over"]),
        ("caught_in",      ["caught", "pinch", "entrap", "in_between", "crush"]),
        ("cut_laceration", ["laceration", "cut", "slice", "saw", "blade"]),
        ("overexertion",   ["overexertion", "strain", "sprain", "lifting", "overexposure"]),
        ("electrical",     ["electr", "shock", "arc", "power line"]),
        ("chemical",       ["chemical", "exposure", "inhalation", "toxic", "hazmat"]),
        ("fire_explosion", ["fire", "explosion", "burn"]),
        ("transport",      ["vehicle", "forklift", "truck", "transport", "mobile"]),
        ("other",          []),
    ]
    for label, kws in rules:
        if any(kw in s for kw in kws):
            return label
    return "other"

df_min["family"] = df_min["eventtitle"].map(default_family_rules)

sep("Distribuci√≥n de familias (simplificadas)")
display(df_min["family"].value_counts())

# SPLIT ESTRATIFICADO ALEATORIO

from sklearn.model_selection import train_test_split

sep("Crear splits estratificados")

X = df_min["final_narrative"]   # texto principal

# üëâ Elige la etiqueta:
# y = df_min["eventtitle"]   # versi√≥n detallada (muchas clases)
y = df_min["family"]          # versi√≥n simplificada (menos clases)

# Train (70%) vs Resto (30%)
X_train, X_temp, y_train, y_temp = train_test_split(
    X, y, test_size=0.30, stratify=y, random_state=RANDOM_STATE
)

# De ese 30%, dividimos en valid (15%) y test (15%)
X_valid, X_test, y_valid, y_test = train_test_split(
    X_temp, y_temp, test_size=0.50, stratify=y_temp, random_state=RANDOM_STATE
)

print("Tama√±os finales:")
print("Train:", X_train.shape[0])
print("Valid:", X_valid.shape[0])
print("Test :", X_test.shape[0])

# Distribuci√≥n de clases en cada split
def dist(y, name):
    print(f"\nDistribuci√≥n {name}:")
    display(y.value_counts(normalize=True).round(3).head(10))

dist(y_train, "train")
dist(y_valid, "valid")
dist(y_test, "test")

# BASELINE Y MODELO

from sklearn.dummy import DummyClassifier
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.pipeline import Pipeline
from sklearn.metrics import classification_report, f1_score, accuracy_score

sep("Baseline (mayoritaria)")
baseline = DummyClassifier(strategy="most_frequent", random_state=RANDOM_STATE)
baseline.fit(X_train, y_train)
pred_va_base = baseline.predict(X_valid)

print("Baseline VALID | acc:", accuracy_score(y_valid, pred_va_base),
      "| f1_macro:", f1_score(y_valid, pred_va_base, average="macro"))

sep("Modelo TF-IDF + LogisticRegression (class_weight balanced)")
pipe = Pipeline([
    ("tfidf", TfidfVectorizer(
        ngram_range=(1,2),
        min_df=3,
        max_df=0.95,
        strip_accents="unicode",
        sublinear_tf=True
    )),
    ("clf", LogisticRegression(
        max_iter=300,
        n_jobs=-1,
        class_weight="balanced",
        solver="saga",
        random_state=RANDOM_STATE
    ))
])

pipe.fit(X_train, y_train)

pred_va = pipe.predict(X_valid)
print("VALID | acc:", accuracy_score(y_valid, pred_va),
      "| f1_macro:", f1_score(y_valid, pred_va, average="macro"))
print("\nReporte VALID:")
print(classification_report(y_valid, pred_va, digits=3))

sep("Evaluaci√≥n en TEST")
pred_te = pipe.predict(X_test)
print("TEST | acc:", accuracy_score(y_test, pred_te),
      "| f1_macro:", f1_score(y_test, pred_te, average="macro"))
print("\nReporte TEST:")
print(classification_report(y_test, pred_te, digits=3))

# MATRIZ DE CONFUSI√ìN Y ERRORES

import matplotlib.pyplot as plt
import numpy as np
from sklearn.metrics import confusion_matrix

labels = sorted(pd.unique(y_train))
cm = confusion_matrix(y_test, pred_te, labels=labels, normalize="true")

plt.figure(figsize=(8,6))
plt.imshow(cm, interpolation="nearest")
plt.title("Matriz de confusi√≥n (TEST, normalizada)")
plt.colorbar()
tick_marks = np.arange(len(labels))
plt.xticks(tick_marks, labels, rotation=45, ha="right")
plt.yticks(tick_marks, labels)
plt.tight_layout()
plt.xlabel("Predicci√≥n"); plt.ylabel("Real")
plt.show()

# Errores m√°s comunes: d√≥nde se confunde y ejemplos
err_df = pd.DataFrame({
    "y_true": y_test.reset_index(drop=True),
    "y_pred": pd.Series(pred_te).reset_index(drop=True),
    "texto": X_test.reset_index(drop=True)
})
err_df = err_df[err_df["y_true"] != err_df["y_pred"]].copy()
print("Errores totales en TEST:", err_df.shape[0])

# Mapa de confusiones top
conf_counts = err_df.groupby(["y_true","y_pred"]).size().sort_values(ascending=False).head(10)
print("\nTop 10 confusiones (y_true ‚Üí y_pred):")
print(conf_counts)

display(err_df.head(10))

# === Curvas de calibraci√≥n multiclase (OVR) usando from_predictions ===
from sklearn.pipeline import make_pipeline
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.svm import LinearSVC
from sklearn.calibration import CalibratedClassifierCV, CalibrationDisplay
from sklearn.metrics import brier_score_loss
import numpy as np
import matplotlib.pyplot as plt

# 1) Pipeline TF-IDF + LinearSVC
pipe_svm = make_pipeline(
    TfidfVectorizer(lowercase=True, ngram_range=(1,2), min_df=3, max_df=0.95),
    LinearSVC(class_weight='balanced', max_iter=300, random_state=42)
)

# 2) Calibraci√≥n (Platt scaling) sobre el pipeline multiclase
svm_calibrado = CalibratedClassifierCV(
    estimator=pipe_svm, method="sigmoid", cv=5
)
svm_calibrado.fit(X_train, y_train)

# 3) Probabilidades y clases (ojo: columnas de y_proba siguen el orden de classes_)
y_proba = svm_calibrado.predict_proba(X_test)   # shape: (n_muestras, n_clases)
classes = svm_calibrado.classes_                # array de etiquetas en el mismo orden que las columnas

# 4) Elegimos 3 clases visibles (aj√∫stalas si quieres otras)
clases_a_graficar = ["fall", "caught_in", "struck_by"]
clases_a_graficar = [c for c in clases_a_graficar if c in classes]

fig, ax = plt.subplots(figsize=(6,6))
for c in clases_a_graficar:
    idx = np.where(classes == c)[0][0]        # columna de esa clase
    y_true_bin = (y_test == c).astype(int)    # OVR: 1 si es la clase c, 0 si no
    y_prob_c   = y_proba[:, idx]              # prob de la clase c

    # ¬°Clave!: usamos from_predictions (no from_estimator) para binario OVR
    CalibrationDisplay.from_predictions(
        y_true=y_true_bin,
        y_prob=y_prob_c,
        n_bins=10,
        strategy='uniform',
        name=f"Clase: {c}",
        ax=ax,
    )

ax.plot([0,1],[0,1],"--",linewidth=1)         # diagonal ideal
ax.set_title("Figura 3. Curvas de calibraci√≥n (Linear SVM calibrado - OVR)")
ax.set_xlabel("Probabilidad media por bin")
ax.set_ylabel("Fracci√≥n positiva")
plt.tight_layout()
plt.show()

# 5) Brier score macro (promedio OVR por clase)
briers = []
for i, c in enumerate(classes):
    y_true_bin = (y_test == c).astype(int)
    y_prob_c   = y_proba[:, i]
    briers.append(brier_score_loss(y_true_bin, y_prob_c))
print("Brier score macro (OVR promedio):", round(float(np.mean(briers)), 4))

# PALABRAS TOP POR CLASE

vec = pipe.named_steps["tfidf"]
clf = pipe.named_steps["clf"]

feature_names = np.array(vec.get_feature_names_out())

def top_terms_per_class(clf, feature_names, k=15):
    for i, cls in enumerate(clf.classes_):
        coef = clf.coef_[i]
        top_idx = np.argsort(coef)[-k:]
        print(f"\nClase: {cls}")
        print(", ".join(feature_names[top_idx][::-1]))

top_terms_per_class(clf, feature_names, k=15)

# GUARDADO DE ARTEFACTOS

import joblib
from pathlib import Path
from sklearn.metrics import accuracy_score, f1_score

model_path = OUT_DIR / "modelo_family_tfidf_logreg.joblib"
joblib.dump(pipe, model_path)

# Guardar reporte de m√©tricas
report_path = OUT_DIR / "resumen_metricas.txt"
with open(report_path, "w") as f:
    f.write("VALID\n")
    f.write(f"accuracy: {accuracy_score(y_valid, pred_va):.4f}\n")
    f.write(f"f1_macro: {f1_score(y_valid, pred_va, average='macro'):.4f}\n\n")
    f.write("TEST\n")
    f.write(f"accuracy: {accuracy_score(y_test, pred_te):.4f}\n")
    f.write(f"f1_macro: {f1_score(y_test, pred_te, average='macro'):.4f}\n")

print(f"Modelo guardado en: {model_path}")
print(f"M√©tricas guardadas en: {report_path}")

# =========================================
# CELDA 12: COMPARACI√ìN CON SVM
# =========================================
from sklearn.svm import LinearSVC

sep("Modelo TF-IDF + Linear SVM")

pipe_svm = Pipeline([
    ("tfidf", TfidfVectorizer(
        ngram_range=(1,2),
        min_df=3,
        max_df=0.95,
        strip_accents="unicode",
        sublinear_tf=True
    )),
    ("clf", LinearSVC(class_weight="balanced", random_state=RANDOM_STATE, max_iter=3000))
])

pipe_svm.fit(X_train, y_train)

pred_va_svm = pipe_svm.predict(X_valid)
print("VALID | acc:", accuracy_score(y_valid, pred_va_svm),
      "| f1_macro:", f1_score(y_valid, pred_va_svm, average="macro"))
print("\nReporte VALID:")
print(classification_report(y_valid, pred_va_svm, digits=3))

sep("Evaluaci√≥n en TEST")
pred_te_svm = pipe_svm.predict(X_test)
print("TEST | acc:", accuracy_score(y_test, pred_te_svm),
      "| f1_macro:", f1_score(y_test, pred_te_svm, average="macro"))
print("\nReporte TEST:")
print(classification_report(y_test, pred_te_svm, digits=3))

# GUARDAR SVM Y M√âTRICAS

import joblib
from sklearn.metrics import accuracy_score, f1_score, classification_report
from pathlib import Path

svm_model_path = OUT_DIR / "modelo_family_tfidf_linearSVM.joblib"
joblib.dump(pipe_svm, svm_model_path)

# Guardar m√©tricas comparadas
cmp_path = OUT_DIR / "metricas_comparacion.csv"
import pandas as pd
cmp_df = pd.DataFrame([
    {"modelo": "Baseline-majority", "partition": "valid", "accuracy": accuracy_score(y_valid, pred_va_base), "f1_macro": f1_score(y_valid, pred_va_base, average="macro")},
    {"modelo": "LogisticRegression", "partition": "valid", "accuracy": accuracy_score(y_valid, pred_va), "f1_macro": f1_score(y_valid, pred_va, average="macro")},
    {"modelo": "LinearSVM", "partition": "valid", "accuracy": accuracy_score(y_valid, pred_va_svm), "f1_macro": f1_score(y_valid, pred_va_svm, average="macro")},
    {"modelo": "Baseline-majority", "partition": "test", "accuracy": accuracy_score(y_test, baseline.predict(X_test)), "f1_macro": f1_score(y_test, baseline.predict(X_test), average="macro")},
    {"modelo": "LogisticRegression", "partition": "test", "accuracy": accuracy_score(y_test, pred_te), "f1_macro": f1_score(y_test, pred_te, average="macro")},
    {"modelo": "LinearSVM", "partition": "test", "accuracy": accuracy_score(y_test, pred_te_svm), "f1_macro": f1_score(y_test, pred_te_svm, average="macro")},
])
cmp_df.to_csv(cmp_path, index=False)

print(f"SVM guardado en: {svm_model_path}")
print(f"M√©tricas comparadas en: {cmp_path}")

# Reporte de clasificaci√≥n en TEST (SVM) para anexos
rep_path = OUT_DIR / "classification_report_svm_test.txt"
with open(rep_path, "w") as f:
    f.write(classification_report(y_test, pred_te_svm, digits=3))
print(f"Reporte TEST SVM guardado en: {rep_path}")

# TABLA + GR√ÅFICO COMPARATIVO

import matplotlib.pyplot as plt

display(cmp_df)

plt.figure(figsize=(8,4))
sub = cmp_df[cmp_df["partition"]=="test"]
plt.bar(sub["modelo"], sub["f1_macro"])
plt.title("Comparaci√≥n F1-macro (TEST)")
plt.ylabel("F1-macro")
plt.tight_layout()
plot_path = OUT_DIR / "comparacion_f1_test.png"
plt.savefig(plot_path, dpi=200)
plt.show()
print(f"Gr√°fico guardado en: {plot_path}")

# CALIBRACI√ìN DE PROBABILIDADES (SVM) COMPATIBLE

import sklearn
from sklearn.calibration import CalibratedClassifierCV
from sklearn.svm import LinearSVC
from sklearn.pipeline import Pipeline
from sklearn.metrics import accuracy_score, f1_score

sep(f"Entrenar SVM calibrado (scikit-learn {sklearn.__version__})")

# Construimos el estimador base (LinearSVC) y elegimos el nombre de par√°metro correcto
svm_base = LinearSVC(class_weight="balanced", random_state=RANDOM_STATE, max_iter=3000)

# Algunos sklearn usan 'estimator', otros 'base_estimator'
try:
    calibrator = CalibratedClassifierCV(estimator=svm_base, cv=3, method="sigmoid")
except TypeError:
    calibrator = CalibratedClassifierCV(base_estimator=svm_base, cv=3, method="sigmoid")

pipe_svm_cal = Pipeline([
    ("tfidf", TfidfVectorizer(
        ngram_range=(1,2), min_df=3, max_df=0.95,
        strip_accents="unicode", sublinear_tf=True
    )),
    ("clf", calibrator)
])

pipe_svm_cal.fit(X_train, y_train)

# M√©tricas con el modelo calibrado
pred_va_cal = pipe_svm_cal.predict(X_valid)
pred_te_cal = pipe_svm_cal.predict(X_test)

print("VALID (calibrado) | acc:", accuracy_score(y_valid, pred_va_cal),
      "| f1_macro:", f1_score(y_valid, pred_va_cal, average="macro"))
print("TEST  (calibrado) | acc:", accuracy_score(y_test, pred_te_cal),
      "| f1_macro:", f1_score(y_test, pred_te_cal, average="macro"))

# INFERENCIA (TOP-K)

import numpy as np

def predict_topk(text, model=pipe_svm_cal, k=3):
    proba = model.predict_proba([text])[0]
    classes = model.classes_
    idx = np.argsort(proba)[-k:][::-1]
    return [(classes[i], float(proba[i])) for i in idx]

# Prueba r√°pida
ejemplo = "employee slipped from a ladder while carrying a box"
print(predict_topk(ejemplo, k=3))

# APP DEMO CON GRADIO

!pip -q install gradio==4.44.0

import gradio as gr

labels = list(pipe_svm_cal.classes_)

def infer_ui(texto):
    if not texto or len(texto.strip()) < 5:
        return "Escribe una descripci√≥n m√°s detallada.", []
    top3 = predict_topk(texto, k=3)
    # Devolvemos predicci√≥n principal y tabla de top3
    pred, prob = top3[0]
    rows = [{"Clase": c, "Probabilidad": f"{p:.3f}"} for c,p in top3]
    return f"Predicci√≥n: {pred} (p‚âà{prob:.3f})", rows

with gr.Blocks() as demo:
    gr.Markdown("## Predicci√≥n de familia de accidente (SVM calibrado)")
    inp = gr.Textbox(label="Descripci√≥n del incidente")
    out_text = gr.Textbox(label="Resultado")
    out_table = gr.Dataframe(headers=["Clase","Probabilidad"], datatype=["str","str"], interactive=False)
    btn = gr.Button("Predecir")
    btn.click(infer_ui, inputs=inp, outputs=[out_text, out_table])

demo.launch(share=False)

# SCORING POR LOTES

import pandas as pd

def score_csv(path_in, text_col, path_out):
    dfb = pd.read_csv(path_in)
    assert text_col in dfb.columns, f"No existe la columna {text_col} en el CSV."
    probs = pipe_svm_cal.predict_proba(dfb[text_col].fillna("").astype(str))
    preds = pipe_svm_cal.predict(dfb[text_col].fillna("").astype(str))
    classes = pipe_svm_cal.classes_
    # Armamos salida con pred y top1 prob
    top1_prob = probs.max(axis=1)
    dfb["pred_family"] = preds
    dfb["pred_prob"] = top1_prob
    dfb.to_csv(path_out, index=False)
    return path_out

# Ejemplo de uso:
# salida = score_csv("/content/mis_nuevos_casos.csv", "description", OUT_DIR / "scored_output.csv")
# print("Generado:", salida)

# ARTEFACTOS PARA EL INFORME

# 1) Guardar matriz de confusi√≥n (SVM calibrado)
from sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix
import matplotlib.pyplot as plt

labels = sorted(pd.unique(y_test))
cm = confusion_matrix(y_test, pred_te_cal, labels=labels, normalize="true")
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)
fig, ax = plt.subplots(figsize=(8,6))
disp.plot(ax=ax, cmap="Blues", values_format=".2f", colorbar=False)
plt.title("Matriz de confusi√≥n (TEST, normalizada) ‚Äî SVM calibrado")
plt.xticks(rotation=45, ha="right")
plt.tight_layout()
cm_path = OUT_DIR / "matriz_confusion_svm_cal_test.png"
plt.savefig(cm_path, dpi=200)
plt.show()
print("Matriz guardada en:", cm_path)

# 2) Guardar top t√©rminos por clase (usamos el SVM no calibrado para coeficientes)
vec = pipe_svm.named_steps["tfidf"]
clf = pipe_svm.named_steps["clf"]
import numpy as np
feat = np.array(vec.get_feature_names_out())
top_terms = {}
for i, cls in enumerate(clf.classes_):
    # LinearSVC no expone coef_ con shape [n_classes, n_features] si one-vs-rest -> s√≠ expone
    coef = clf.coef_[i]
    idx = np.argsort(coef)[-20:]
    top_terms[cls] = list(feat[idx][::-1])

terms_path = OUT_DIR / "top_terms_per_class.json"
import json
with open(terms_path, "w") as f:
    json.dump(top_terms, f, indent=2)
print("Top t√©rminos por clase guardados en:", terms_path)

# UMBRALES POR CLASE Y REGLA ANTI-"OTHER"

import numpy as np

# 1) umbrales por clase (aj√∫stalos con sentido com√∫n tras mirar errores)
CLASS_THRESHOLDS = {
    "transport": 0.35,
    "cut_laceration": 0.30,
    "fire_explosion": 0.30,
    "overexertion": 0.25,
    "struck_by": 0.25,
    "caught_in": 0.25,
    "chemical": 0.25,
    "electrical": 0.25,
    "fall": 0.25,
    "other": 0.40,   # ‚Üê m√°s exigente para no caer en "other" por defecto
}

# 2) funci√≥n de predicci√≥n con top-k, umbrales y regla contra "other"
def predict_with_thresholds(text, model=pipe_svm_cal, k=3, thresholds=CLASS_THRESHOLDS):
    proba = model.predict_proba([text])[0]
    classes = model.classes_
    order = np.argsort(proba)[::-1]  # descendente
    ranked = [(classes[i], float(proba[i])) for i in order[:k]]

    # candidato top1
    c1, p1 = ranked[0]

    # regla anti-"other": si top1 es other y top2 est√° razonable, elegir top2
    if c1 == "other":
        c2, p2 = ranked[1]
        # si top2 supera su umbral y no est√° muy lejos de "other"
        if p2 >= thresholds.get(c2, 0.25) and (p1 - p2) <= 0.08:
            c1, p1 = c2, p2

    # umbral por clase: si top1 no llega a su umbral, degradar a siguiente v√°lida
    if p1 < thresholds.get(c1, 0.25):
        for (c, p) in ranked[1:]:
            if p >= thresholds.get(c, 0.25) and c != "other":
                c1, p1 = c, p
                break

    return c1, p1, ranked

# Prueba r√°pida
txt_prueba = "employee was reversing a forklift and was struck in the loading dock"
pred, prob, topk = predict_with_thresholds(txt_prueba, k=3)
print("Predicci√≥n:", pred, f"(p‚âà{prob:.3f})")
print("Top3:", topk)

# RECOMENDACIONES PPE Y CONTROLES

PPE_CONTROLS = {
    "fall": {
        "ppe": ["Casco de seguridad", "Arn√©s con l√≠nea de vida", "Calzado antideslizante"],
        "controles": ["Barandillas/redes", "Se√±alizaci√≥n y orden", "Inspecci√≥n de escaleras/andamios"]
    },
    "caught_in": {
        "ppe": ["Guantes anticorte", "Ropa ajustada", "Protecci√≥n ocular"],
        "controles": ["Resguardos de m√°quina", "Bloqueo/etiquetado (LOTO)", "Zonas de exclusi√≥n"]
    },
    "struck_by": {
        "ppe": ["Casco", "Chaleco alta visibilidad", "Protecci√≥n ocular"],
        "controles": ["Rutas peatonales", "Spotters en maniobras", "Mantenimiento de veh√≠culos"]
    },
    "chemical": {
        "ppe": ["Guantes qu√≠micos", "Gafas/screen facial", "Respirador seg√∫n SDS"],
        "controles": ["Ventilaci√≥n", "Duchas/lavado ocular", "Almacenaje y etiquetado (SDS)"]
    },
    "electrical": {
        "ppe": ["Guantes diel√©ctricos", "Calzado aislante", "Pantalla facial ARC"],
        "controles": ["LOTO el√©ctrico", "Detectores de tensi√≥n", "Trabajos en BT/AT con PTW"]
    },
    "overexertion": {
        "ppe": ["Faja (si pol√≠tica lo permite)", "Guantes de agarre"],
        "controles": ["Ayudas mec√°nicas", "Rotaci√≥n de tareas", "Formaci√≥n en levantamiento seguro"]
    },
    "fire_explosion": {
        "ppe": ["Ropa ign√≠fuga", "Guantes resistentes al calor", "Gafas/pantalla facial"],
        "controles": ["Permiso de trabajo en caliente", "Extintores y cortafuegos", "Control de atm√≥sferas explosivas"]
    },
    "transport": {
        "ppe": ["Chaleco alta visibilidad", "Calzado de seguridad"],
        "controles": ["Separaci√≥n peat√≥n‚Äëveh√≠culo", "Se√±alistas", "L√≠mites de velocidad"]
    },
    "cut_laceration": {
        "ppe": ["Guantes anticorte", "Mangas de protecci√≥n", "Gafas"],
        "controles": ["Cuchillas de seguridad", "Resguardos", "Buenas pr√°cticas de uso"]
    },
    "other": {
        "ppe": ["Protecci√≥n est√°ndar seg√∫n tarea"],
        "controles": ["Evaluaci√≥n espec√≠fica del riesgo"]
    },
}

def enrich_recommendations(family):
    rec = PPE_CONTROLS.get(family, PPE_CONTROLS["other"])
    return rec["ppe"], rec["controles"]

# Demo
print(enrich_recommendations("struck_by"))

# APP GRADIO V2 (UMBRAL + PPE)

import gradio as gr

def infer_ui_v2(texto):
    if not texto or len(texto.strip()) < 10:
        return "Introduce una descripci√≥n m√°s detallada.", [], [], []
    fam, p, top3 = predict_with_thresholds(texto, k=3)
    ppe, controls = enrich_recommendations(fam)
    rows = [{"Clase": c, "Probabilidad": f"{prob:.3f}"} for c, prob in top3]
    return f"Predicci√≥n: {fam} (p‚âà{p:.3f})", rows, ppe, controls

with gr.Blocks() as demo2:
    gr.Markdown("## Predicci√≥n de familia de accidente (SVM calibrado + umbrales)")
    inp = gr.Textbox(label="Descripci√≥n del incidente")
    out_pred = gr.Textbox(label="Resultado")
    out_top = gr.Dataframe(headers=["Clase","Probabilidad"], datatype=["str","str"], interactive=False)
    out_ppe = gr.HighlightedText(label="PPE recomendado")
    out_ctl = gr.HighlightedText(label="Controles sugeridos")
    btn = gr.Button("Predecir")
    btn.click(infer_ui_v2, inputs=inp, outputs=[out_pred, out_top, out_ppe, out_ctl])

demo2.launch(share=False)  # pon share=True si necesitas URL p√∫blica

# RECHAZO POR BAJA CONFIANZA

import numpy as np
from sklearn.metrics import accuracy_score, f1_score

def apply_reject(texts, y_true, conf_thr=0.30, model=pipe_svm_cal):
    probs = model.predict_proba(texts)
    idx = probs.argmax(1)
    pmax = probs.max(1)
    y_pred = model.classes_[idx]
    mask = pmax >= conf_thr
    accepted = np.where(mask)[0]
    rejected = np.where(~mask)[0]
    acc = accuracy_score(np.array(y_true)[accepted], y_pred[accepted]) if len(accepted)>0 else np.nan
    f1m = f1_score(np.array(y_true)[accepted], y_pred[accepted], average="macro") if len(accepted)>0 else np.nan
    return {"accepted": len(accepted), "rejected": len(rejected), "acc_accept": acc, "f1_accept": f1m}

res_val = apply_reject(X_valid, y_valid, conf_thr=0.30)
res_test = apply_reject(X_test,  y_test,  conf_thr=0.30)
print("VALID:", res_val)
print("TEST :", res_test)

import os, glob, json, joblib
from pathlib import Path

BASE = Path("/content/salidas_osha")
print("Existe carpeta salidas_osha:", BASE.exists())
print("\nListado /content/salidas_osha:")
for p in sorted(BASE.glob("*")):
    print(" -", p.name)

print("\nJoblibs encontradas:")
print(glob.glob("/content/salidas_osha/*.joblib"))

from google.colab import files
import shutil
shutil.make_archive("salidas_osha", 'zip', "/content/salidas_osha")
files.download("salidas_osha.zip")

# Commented out IPython magic to ensure Python compatibility.
## 0) Identidad git
!git config --global user.name "laurapereap"
!git config --global user.email "laurapepossos@gmail.com"

# 1) Ir al repo
# %cd /content/zalemo-osha-streamlit

# 2) Crear carpeta de artefactos y copiar desde tu carpeta de trabajo
!mkdir -p salidas_osha
!cp -v /content/salidas_osha/* salidas_osha/ 2>/dev/null || echo "‚ö†Ô∏è Revisa que /content/salidas_osha tenga archivos"

# 3) Permitir que se suban artefactos (anula ignores globales)
!printf '\n# --- allow model artifacts ---\n!salidas_osha/*.joblib\n!salidas_osha/*.csv\n!salidas_osha/*.png\n!salidas_osha/*.txt\n!salidas_osha/*.json\n' >> .gitignore

# 4) Commit
!git add -A
!git status
!git commit -m "Add model artifacts for Streamlit app" || echo "Nada que commitear"